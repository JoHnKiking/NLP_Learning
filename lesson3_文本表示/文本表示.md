## 常见任务
### 文本分类
### 序列标注
### 文本生成
### 信息抽取
### 文本转换


## 分词
### 1.英文分词
#### 词级分词
将文本按词语进行切分
#### 字符级分词
以单个字符为最小单位进行分词
#### 子词级分词
将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段
##### BPE（Byte Pair Encoding）
首先将语料中的词汇拆分为单个字符，构建初始词表；然后迭代地统计语料中出现频率最高的相邻字符对，将其合并为新的子词单元，并加入词表。这个过程持续进行，直到词表大小达到预设上限。
##### WordPiece
##### Unigram Language Model

### 2.中文分词
#### 词级分词
将中文文本按照完整词语进行切分
#### 字符级分词
将文本按照单个汉字进行切分
#### 子词级分词
以汉字为基本单位，通过学习语料中高频的字组合

### 3.分词工具
#### 一类是基于词典或模型的传统方法，主要以“词”为单位进行切分
##### jieba
###### 精确模式
试图将句子最精确地切开
- jieba.cut(text) # 返回一个生成器
- jieba.lcut(text) # 返回一个列表
###### 全模式
把句子中所有的可以成词的词语都扫描出来
- ieba.cut(text, cut_all=True)  # 返回一个生成器
- ieba.lcut(text, cut_all=True)  # 返回一个列表
###### 搜索引擎模式
在精确模式基础上，对长词进一步切分，适合用于搜索引擎分词
- jieba.cut_for_search(text)  # 返回一个生成器
- jieba.lcut_for_search(text)  # 返回一个列表
###### 自定义词典
- 支持用户自定义词典，以便包含 jieba 词库里没有的词，用于增强特定领域词汇的识别能力
- 一个词占一行，每一行分三部分：
- 词语
- 词频（可省略，词频决定某个词在分词时的优先级。词频越高被优先切分出来的概率越大）
- 词性标签（可省略，不影响分词结果）

##### HanLp
#### 另一类是基于子词建模算法（如BPE）的方式，从数据中自动学习高频字组合，构建子词词表。
##### Hugging Face Tokenizer
##### SetencePiece
##### titoken


## 词表示
- 文本被转换为一系列的 token（词、子词或字符），本身对计算机而言是不可计算的。必须将这些 token 转换为计算机可以识别和操作的数值形式，这一步就是所谓的词表示（word representation）
- 但它无法体现词与词之间的语义关系，且随着词表规模的扩大，向量维度会迅速膨胀，导致计算效率低下

### One-hot 编码
它将词汇表中的每个词映射为一个稀疏向量，向量的长度等于整个词表的大小。该词在对应的位置为 1，其他位置为 0
### 语义化词向量
#### Word2Vec
##### 概述
Word2Vec的设计理念源自“分布假设”——即一个词的含义由它周围的词决定。构建了一个简洁的神经网络模型，通过学习词与上下文之间的关系，自动为每个词生成一个能够反映语义特征的向量表示。
##### 原理
###### 数据集：
不依赖人工标注，而是直接利用大规模原始文本（如书籍、新闻、网页等）作为数据源，从中自动构造训练样本。首先需要对原始文本进行分词，将连续文本转换为 token 序列。此外，模型无法直接处理文本符号，训练时仍需将词语转换为 one-hot 编码，以便作为模型的输入和输出进行计算。
###### Skip-Gram：根据中间词预测上下文
- 1.输入中心词:（地铁）“地铁”用 one-hot 向量表示
- 2.查找词向量: 与参数矩阵Wi相乘，取出“地铁”对应的词向量。（Wi实际上就是词向量矩阵，每一行表示一个词的向量）
- 3.预测上下文: 将中心词向量与参数矩阵Wo相乘，得到对整个词表的预测得分。
- 4.Softmax输出: 得分通过 Softmax 转为概率分布，表示各词作为上下文的可能性。
- 5.计算损失: 与真实上下文词“乘坐”、“上班”进行比对，计算交叉熵损失并求和，得到总损失。
- 之后在进行反向传播时，参数矩阵中的“地铁”对应的词向量就会被更新，模型通过这个过程不断的进行学习，最终便能得到具有语义的词向量。

###### CBOW：根据上下文预测中间词
- 1.输入上下文词:（乘坐、上班）每个词用 one-hot 向量表示。
- 2.查找词向量: 每个 one-hot 向量与参数矩阵Wi相乘，查出对应的词向量。（Wi实际上就是词向量矩阵，每一行表示一个词的向量）
- 3.平均上下文向量: 将多个上下文词向量取平均，得到一个整体的上下文表示。
- 4.预测中心词: 将平均后的上下文向量与参数矩阵Wo相乘，得到对整个词表的预测得分。
- 5.Softmax输出: 将得分输入Softmax，得到每个词作为中心词的概率分布。
- 6.计算损失: 将预测结果与真实中心词“地铁”的one-hot向量进行比对，计算交叉熵损失。
- 之后在进行反向传播时，参数矩阵中“乘坐”和“上班”对应的词向量就会被更新。模型通过不断训练，逐步优化这些向量，最终便能得到具有语义的词向量

#### 获取Word2Vec词向量
##### 使用公开词向量
- 公开的中文词向量，可从 `https://github.com/Embedding/Chinese-Word-Vectors` 下载
- 第一行记录基本信息，包括两个整数，分别表示总词数和词向量维度。从第二行起，每一行表示一个词及其对应的词向量，格式为：词 + 向量的各个维度值。所有内容通过空格分隔
- 可使用KeyedVectors.load_word2vec_format() 加载上述词向量文件
- 查看词向量维度：print(model.vector_size)
- 查看某个词的向量：print(model['地铁'])
- 查看两个向量的相似度：similarity = model.similarity('地铁', '公交')

##### 自行训练词向量
- 准备语料
- 训练模型
- 保存词向量
- 加载词向量
